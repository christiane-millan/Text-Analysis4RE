{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del texto\n",
    "\n",
    "Los datos a utilizar pueden contener caracteres o tokens innecesarios, por lo que deben ser eliminados antes de realizar alguna operación como la tokenización o normalización. Por ejemplo, etiquetas HTML, XML o JSON. \n",
    "\n",
    "Existen distintas formas de limpiar el texto con funciones como `clean_html()` de `nltk` o con la librería `BeautifulSoup`. También se pueden utilizar expresiones regulares, xpath o la librería lxml.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '*',\n",
      "   '*',\n",
      "   'lot',\n",
      "   '*',\n",
      "   '*',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.'],\n",
      "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) for text in corpus] \n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación caracteres especiales\n",
    "\n",
    "Una tarea previa a la normalización del texto es eliminar los caracteres especiales. Por ejemplo símbolos especiales, signos de puntuación. Este paso regularmente es ejecutado antes o después de la tokenización. La razón de este paso es eliminar elementos que no tienen significado cuando se analiza el texto, o se extraen características o información basada en NLP o ML. \n",
    "\n",
    "El siguiente código muestra la eleminición de caracteres especiales después de la tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) \n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x15ceb0190>,\n",
      " <filter object at 0x15ceb0730>,\n",
      " <filter object at 0x15ceb0bb0>]\n"
     ]
    }
   ],
   "source": [
    "filtered_list_1 =  [filter(None,[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens]) for sentence_tokens in token_list]\n",
    "pprint(list(filtered_list_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo `string.puntuation` consiste en todos los símbolos y caracteres especiales, y crear patrones de expresiones regulares de estos. La función `filter` ayuda a remover los tokens vacios obtenidos después de revomer token de caracteres especiales utilizando el método `reg sub`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print(filtered_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "print(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expasión de contracciones\n",
    "\n",
    "Las contracciones son versiónes cortas de palabras o silabas. En inglés los constracciones son creadas removiendo vocales de la palabra. Por ejemplo, is not - isn't, will not - won't. Usualmente las constracciones son evitadas en escritura formal, pero en la escritura informal son muy comunes. Existen varias formas de contracción que esta ligadas al tipo de verbo auxiliar que da una forma de contracción normal, negada u otras contracciones coloquiales especiales, algunas de ellas no implican auxiliares. \n",
    "\n",
    "Las contracciones implican un problema en NLP y análisis de texto, ya que contiene el caracter especial apostrofe en la palabra. Además, se tiene dos o más palabras representadas en la contracción, espo amplia las formas que se puede tokenizar una palabra. Idealmente, se puede proponer un mapeo de contracciones y sus correspondientes expansiones.\n",
    "\n",
    "Se puede crear un archivo `contractions.py` en un diccionario de Python.\n",
    "\n",
    "````Python\n",
    "CONTRACTION_MAP = {\n",
    "\"isn't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "````\n",
    "Se debe considera que algunas de las contracciones tienen multiplés formas. Por ejemplo, *you'll* puede indicar *you will* o *you shall*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
    "\n",
    "print(expanded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mayúsculas y minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras vacías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expanded_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cmillan/Documents/1 Projects/UTM-MIS  DA4RE 2024A/Text-Analysis4RE/L002-Processing-understanding/code/text_normalization.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cmillan/Documents/1%20Projects/UTM-MIS%20%20DA4RE%202024A/Text-Analysis4RE/L002-Processing-understanding/code/text_normalization.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m expanded_corpus_tokens \u001b[39m=\u001b[39m [tokenize_text(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m expanded_corpus]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmillan/Documents/1%20Projects/UTM-MIS%20%20DA4RE%202024A/Text-Analysis4RE/L002-Processing-understanding/code/text_normalization.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filtered_list_3 \u001b[39m=\u001b[39m  [[remove_stopwords(tokens) \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m sentence_tokens] \u001b[39mfor\u001b[39;00m sentence_tokens \u001b[39min\u001b[39;00m expanded_corpus_tokens]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmillan/Documents/1%20Projects/UTM-MIS%20%20DA4RE%202024A/Text-Analysis4RE/L002-Processing-understanding/code/text_normalization.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(filtered_list_3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expanded_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus]\n",
    "\n",
    "filtered_list_3 =  [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]\n",
    "print(filtered_list_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt4re2024A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
