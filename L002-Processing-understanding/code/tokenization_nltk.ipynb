{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización\n",
    "\n",
    "# Tokenización de sentencias\n",
    "\n",
    "EL framework NLTK provee varias interfaces  para realizar la tokenización de sentencias. Más información [aquí](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "Nota: Para realizar la instalación de NLTK mediante conda, ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "conda install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/cmillan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = 'We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n"
     ]
    }
   ],
   "source": [
    "# Caracteres totales en 'alice wonderland'\n",
    "print(len(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los primeros 100 caracteres en el corpus\n",
    "print(alice[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `nltk.sent_tokenize` es la función por defecto para la tokenización de sentencias que nltk recomienda. \n",
    "\n",
    "Nota: Utiliza la clase PunktSentenceTokenizer la cual ha sido entrenada con muchos modelos de lenguajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cmillan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice: \n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice: ')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences : \n",
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences : ')\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible tokenizar otros lenguajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /Users/cmillan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('europarl_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "\n",
    "# Total characters in the corpus \n",
    "print(len(german_text))\n",
    "# First 100 characters in the corpus\n",
    "print(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "# verify the type of german_tokenizer should be PunktSentenceTokenizer\n",
    "print(type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "print(german_sentences_def == german_sentences)\n",
    "\n",
    "# print first 5 sentences of the corpus\n",
    "for sent in german_sentences[0:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible tokenizar sentencias pertenecientes a distintos lenguajes de dos diferentes formas. \n",
    "\n",
    "1. Mediante la clase PunktSentenceTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Al utilizar una instancia de la clase RegexpTokenizer, donde se utiliza una expresión regular específica basada en patrones para segmentar las oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "\n",
    "regex_st = nltk.tokenize.RegexpTokenizer( pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt4re2024A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
